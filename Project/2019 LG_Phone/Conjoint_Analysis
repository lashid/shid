import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.api as sm
from sklearn import preprocessing
import matplotlib.pyplot as plt

rank_data=pd.read_csv('C:/Users/jinha/Downloads/survey_result_2_1.csv', index_col=0)
data=pd.DataFrame()
data=rank_data

conjoint_data = pd.get_dummies(rank_data,columns =['camera','battery','memory','CPU'])
print(conjoint_data.head())

fullNames = {"camera_1": "camera_good","camera_2": "camera_normal",'camera_3':'camera_bad','battery_1':'battery_good','battery_2':'battery_normal','battery_3':'battery_bad','memory_1':'memory_good','memory_2':'memory_normal','memory_3':'memory_bad','CPU_1':'CPU_good','CPU_2':'CPU_normal','CPU_3':'CPU_bad'
          }

conjoint_data.rename(columns=fullNames, inplace=True)
conjoint_data = sm.add_constant(conjoint_data)

y=rank_data.index
res = sm.OLS(y, conjoint_data, family=sm.families.Binomial()).fit()
print(res.summary())

df_res = pd.DataFrame({
    'param_name': res.params.keys()
    , 'param_w': res.params.values
    , 'pval': res.pvalues
})
# adding field for absolute of parameters
df_res['abs_param_w'] = np.abs(df_res['param_w'])
# marking field is significant under 95% confidence interval
df_res['is_sig_95'] = (df_res['pval'] < 0.05)
# constructing color naming for each param
df_res['c'] = ['blue' if x else 'red' for x in df_res['is_sig_95']]

# make it sorted by abs of parameter value
df_res = df_res.sort_values(by='abs_param_w', ascending=True)

f, ax = plt.subplots(figsize=(14, 8))
plt.title('Part Worth')
pwu = df_res['param_w']
xbar = np.arange(len(pwu))
plt.barh(xbar, pwu, color=df_res['c'])
plt.yticks(xbar, labels=df_res['param_name'])
plt.show()

range_per_feature = dict()
for key, coeff in res.params.items():
    sk = key.split('_')
    feature = sk[0]
    if len(sk) == 1:
        feature = key
    if feature not in range_per_feature:
        range_per_feature[feature] = list()

    range_per_feature[feature].append(coeff)

# importance per feature is range of coef in a feature
# while range is simply max(x) - min(x)
importance_per_feature = {
    k: max(v) - min(v) for k, v in range_per_feature.items()
}

# compute relative importance per feature
# or normalized feature importance by dividing
# sum of importance for all features
total_feature_importance = sum(importance_per_feature.values())
relative_importance_per_feature = {
    k: 100 * round(v/total_feature_importance, 3) for k, v in importance_per_feature.items()
}
alt_data = pd.DataFrame(
    list(importance_per_feature.items()),
    columns=['attr', 'importance']
).sort_values(by='importance', ascending=False)


f, ax = plt.subplots(figsize=(12, 8))
xbar = np.arange(len(alt_data['attr']))
plt.title('Importance')
plt.barh(xbar, alt_data['importance'])
for i, v in enumerate(alt_data['importance']):
    ax.text(v , i + .25, '{:.2f}'.format(v))
plt.ylabel('attributes')
plt.xlabel('% importance')
plt.yticks(xbar, alt_data['attr'])
plt.show()

alt_data = pd.DataFrame(
    list(relative_importance_per_feature.items()),
    columns=['attr', 'relative_importance (pct)']
).sort_values(by='relative_importance (pct)', ascending=False)


f, ax = plt.subplots(figsize=(12, 8))
xbar = np.arange(len(alt_data['attr']))
plt.title('Relative importance / Normalized importance')
plt.barh(xbar, alt_data['relative_importance (pct)'])
for i, v in enumerate(alt_data['relative_importance (pct)']):
    ax.text(v , i + .25, '{:.2f}%'.format(v))
plt.ylabel('attributes')
plt.xlabel('% relative importance')
plt.yticks(xbar, alt_data['attr'])
plt.show()

X = conjoint_data
Y = rank_data.index
linearRegression = sm.GLM(Y, X).fit()
print(linearRegression.summary())

conjoint_attributes=['camera_good','camera_normal','camera_bad','battery_good','battery_normal','battery_bad','memory_good','memory_normal','memory_bad','CPU_good','CPU_normal','CPU_bad']

level_name = []
part_worth = []
part_worth_range = []
end = 1
for item in conjoint_attributes:
    nlevels = len(list(set(conjoint_data[item])))
    level_name.append(list(set(conjoint_data[item])))
    begin = end
    end = begin + nlevels - 1
    new_part_worth = list(linearRegression.params[begin:end])
    new_part_worth.append((-1) * sum(new_part_worth))
    part_worth_range.append(max(new_part_worth) - min(new_part_worth))
    part_worth.append(new_part_worth)
    # end set to begin next iteration

print(part_worth)
print(new_part_worth)

attribute_importance = []
for item in part_worth_range:
    attribute_importance.append(round(100 * (item / sum(part_worth_range)),2))


effect_name_dict = {"camera_good": "camera_good","camera_normal": "camera_normal",'camera_bad':'camera_bad','battery_good':'battery_good','battery_normal':'battery_normal','battery_bad':'battery_bad','memory_good':'memory_good','memory_normal':'memory_normal','memory_bad':'memory_bad','CPU_good':'CPU_good','CPU_normal':'CPU_normal','CPU_bad':'CPU_bad'
          }


#print out parthworth's for each level
estimates_of_choice = []
index = 0
for item in conjoint_attributes :
    print ("\n Attribute : " , effect_name_dict[item])
    print ("\n Importance : " , attribute_importance[index])
    print('    Level Part-Worths')
    for level in range(len(level_name[index])):
        print('       ',level_name[index][level], part_worth[index][level])
    index = index + 1

df2=pd.read_csv('C:/Users/jinha/Downloads/survey_result_2_2.csv')

utility_scores = df2.values.sum(axis=1)
max_utility = np.argmax(utility_scores)
print("The index of combination combination with hightest sum of utility scores is")
print(df2.loc[max_utility])

total_utility=0
c= 100/(12*20)
for item in utility_scores:
    total_utility = total_utility + np.exp(c*item)

for item in utility_scores:
    probabilty = np.exp(c*item)/total_utility
    itemindex = np.where(utility_scores==item)

    print('Market share of profile %s is %s ', itemindex, probabilty*100)
